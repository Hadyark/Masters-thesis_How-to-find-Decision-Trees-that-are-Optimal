{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from numpy import round_\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from dl85 import DL85Predictor\n",
    "from dl85 import DL85Classifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import MyCode.utils as utils\n",
    "import graphviz\n",
    "from MyCode import relabeling\n",
    "from MyCode import relabeling_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"dataset_perso/Lawsuit.csv\")\n",
    "FILE_NAME = \"lawsuit_k_supp2_relab\"\n",
    "\n",
    "del df['ID']\n",
    "salary_mean = np.mean(np.array(df[\"Sal94\"].tolist(), float))\n",
    "for i in range(0,len(df)):\n",
    "    if df.at[i, \"Sal94\"] >= salary_mean:\n",
    "        df.at[i, \"Salary_mean\"] = 1\n",
    "    else:\n",
    "        df.at[i, \"Salary_mean\"] = 0\n",
    "del df[\"Sal94\"]\n",
    "del df[\"Sal95\"]\n",
    "col = ['Rank', 'Dept']\n",
    "df = df.drop(['Prate', 'Exper'], axis=1)\n",
    "df = pd.get_dummies(df, columns=col)\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    if df.at[i, \"Gender\"] == 1:\n",
    "        df.at[i, \"Gender\"] = 0\n",
    "    else:\n",
    "        df.at[i, \"Gender\"] = 1\n",
    "\n",
    "\n",
    "X = df.loc[:, ~df.columns.isin(['Gender', 'Salary_mean'])]\n",
    "y = df['Salary_mean']\n",
    "sensitive = df['Gender']\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = utils.train_test_split(1, X, y, sensitive)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "           k  depth  random_state    clf.error_  clf.accuracy_  accuracy_pred  \\\n0          0      1             1     51.000000       0.755981       0.755981   \n1          1      1             1     51.325619       0.754423       0.755981   \n2          5      1             1     52.628094       0.748191       0.755981   \n3         10      1             1     54.256191       0.740401       0.755981   \n4         50      1             1     67.280952       0.678082       0.755981   \n...      ...    ...           ...           ...            ...            ...   \n3460     250      7            99    107.093262       0.487592       0.918660   \n3461     500      7            99    197.186523       0.056524       0.918660   \n3462    1000      7            99    377.373047      -0.805613       0.918660   \n3463    5000      7            99   1818.865356      -7.702705       0.918660   \n3464  100000      7            99  36054.304688    -171.508636       0.918660   \n\n      accuracy_test  sum_misclassified_train  \\\n0          0.730769                       51   \n1          0.730769                       51   \n2          0.730769                       51   \n3          0.730769                       51   \n4          0.730769                       51   \n...             ...                      ...   \n3460       0.826923                       17   \n3461       0.826923                       17   \n3462       0.826923                       17   \n3463       0.846154                       17   \n3464       0.826923                       17   \n\n      sum_discrimination_additive_train_abs  \\\n0                                  0.156476   \n1                                  0.156476   \n2                                  0.156476   \n3                                  0.156476   \n4                                  0.156476   \n...                                     ...   \n3460                               0.286508   \n3461                               0.286508   \n3462                               0.286508   \n3463                               0.289806   \n3464                               0.286508   \n\n      sum_discrimination_additive_train_pred_abs  \\\n0                                       0.325619   \n1                                       0.325619   \n2                                       0.325619   \n3                                       0.325619   \n4                                       0.325619   \n...                                          ...   \n3460                                    0.360373   \n3461                                    0.360373   \n3462                                    0.360373   \n3463                                    0.360373   \n3464                                    0.360373   \n\n      sum_discrimination_additive_train  \\\n0                              0.156476   \n1                              0.156476   \n2                              0.156476   \n3                              0.156476   \n4                              0.156476   \n...                                 ...   \n3460                           0.286508   \n3461                           0.286508   \n3462                           0.286508   \n3463                           0.289806   \n3464                           0.286508   \n\n      sum_discrimination_additive_train_pred  discrimination_train  \\\n0                                   0.325619              0.325619   \n1                                   0.325619              0.325619   \n2                                   0.325619              0.325619   \n3                                   0.325619              0.325619   \n4                                   0.325619              0.325619   \n...                                      ...                   ...   \n3460                                0.360373              0.360373   \n3461                                0.360373              0.360373   \n3462                                0.360373              0.360373   \n3463                                0.360373              0.360373   \n3464                                0.360373              0.360373   \n\n      discrimination_train_pred  discrimination_test  duration  min_supp  \\\n0                      0.156476             0.266667  0.012932         2   \n1                      0.156476             0.266667  0.011767         2   \n2                      0.156476             0.266667  0.010726         2   \n3                      0.156476             0.266667  0.011313         2   \n4                      0.156476             0.266667  0.016142         2   \n...                         ...                  ...       ...       ...   \n3460                   0.286508             0.063796  3.914124         2   \n3461                   0.286508             0.063796  3.763764         2   \n3462                   0.286508             0.063796  4.787441         2   \n3463                   0.289806             0.033493  5.107243         2   \n3464                   0.286508             0.063796  5.600595         2   \n\n                                              clf.tree_  \n0     {'feat': 10, 'left': {'value': 1, 'error': 0.0...  \n1     {'feat': 10, 'left': {'value': 1, 'error': 0.1...  \n2     {'feat': 10, 'left': {'value': 1, 'error': 0.7...  \n3     {'feat': 10, 'left': {'value': 1, 'error': 1.5...  \n4     {'feat': 10, 'left': {'value': 1, 'error': 7.8...  \n...                                                 ...  \n3460  {'feat': 5, 'left': {'feat': 0, 'left': {'feat...  \n3461  {'feat': 5, 'left': {'feat': 0, 'left': {'feat...  \n3462  {'feat': 5, 'left': {'feat': 0, 'left': {'feat...  \n3463  {'feat': 1, 'left': {'feat': 6, 'left': {'valu...  \n3464  {'feat': 1, 'left': {'feat': 4, 'left': {'feat...  \n\n[3465 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>k</th>\n      <th>depth</th>\n      <th>random_state</th>\n      <th>clf.error_</th>\n      <th>clf.accuracy_</th>\n      <th>accuracy_pred</th>\n      <th>accuracy_test</th>\n      <th>sum_misclassified_train</th>\n      <th>sum_discrimination_additive_train_abs</th>\n      <th>sum_discrimination_additive_train_pred_abs</th>\n      <th>sum_discrimination_additive_train</th>\n      <th>sum_discrimination_additive_train_pred</th>\n      <th>discrimination_train</th>\n      <th>discrimination_train_pred</th>\n      <th>discrimination_test</th>\n      <th>duration</th>\n      <th>min_supp</th>\n      <th>clf.tree_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>51.000000</td>\n      <td>0.755981</td>\n      <td>0.755981</td>\n      <td>0.730769</td>\n      <td>51</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.266667</td>\n      <td>0.012932</td>\n      <td>2</td>\n      <td>{'feat': 10, 'left': {'value': 1, 'error': 0.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>51.325619</td>\n      <td>0.754423</td>\n      <td>0.755981</td>\n      <td>0.730769</td>\n      <td>51</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.266667</td>\n      <td>0.011767</td>\n      <td>2</td>\n      <td>{'feat': 10, 'left': {'value': 1, 'error': 0.1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52.628094</td>\n      <td>0.748191</td>\n      <td>0.755981</td>\n      <td>0.730769</td>\n      <td>51</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.266667</td>\n      <td>0.010726</td>\n      <td>2</td>\n      <td>{'feat': 10, 'left': {'value': 1, 'error': 0.7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10</td>\n      <td>1</td>\n      <td>1</td>\n      <td>54.256191</td>\n      <td>0.740401</td>\n      <td>0.755981</td>\n      <td>0.730769</td>\n      <td>51</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.266667</td>\n      <td>0.011313</td>\n      <td>2</td>\n      <td>{'feat': 10, 'left': {'value': 1, 'error': 1.5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50</td>\n      <td>1</td>\n      <td>1</td>\n      <td>67.280952</td>\n      <td>0.678082</td>\n      <td>0.755981</td>\n      <td>0.730769</td>\n      <td>51</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.325619</td>\n      <td>0.325619</td>\n      <td>0.156476</td>\n      <td>0.266667</td>\n      <td>0.016142</td>\n      <td>2</td>\n      <td>{'feat': 10, 'left': {'value': 1, 'error': 7.8...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3460</th>\n      <td>250</td>\n      <td>7</td>\n      <td>99</td>\n      <td>107.093262</td>\n      <td>0.487592</td>\n      <td>0.918660</td>\n      <td>0.826923</td>\n      <td>17</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.063796</td>\n      <td>3.914124</td>\n      <td>2</td>\n      <td>{'feat': 5, 'left': {'feat': 0, 'left': {'feat...</td>\n    </tr>\n    <tr>\n      <th>3461</th>\n      <td>500</td>\n      <td>7</td>\n      <td>99</td>\n      <td>197.186523</td>\n      <td>0.056524</td>\n      <td>0.918660</td>\n      <td>0.826923</td>\n      <td>17</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.063796</td>\n      <td>3.763764</td>\n      <td>2</td>\n      <td>{'feat': 5, 'left': {'feat': 0, 'left': {'feat...</td>\n    </tr>\n    <tr>\n      <th>3462</th>\n      <td>1000</td>\n      <td>7</td>\n      <td>99</td>\n      <td>377.373047</td>\n      <td>-0.805613</td>\n      <td>0.918660</td>\n      <td>0.826923</td>\n      <td>17</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.063796</td>\n      <td>4.787441</td>\n      <td>2</td>\n      <td>{'feat': 5, 'left': {'feat': 0, 'left': {'feat...</td>\n    </tr>\n    <tr>\n      <th>3463</th>\n      <td>5000</td>\n      <td>7</td>\n      <td>99</td>\n      <td>1818.865356</td>\n      <td>-7.702705</td>\n      <td>0.918660</td>\n      <td>0.846154</td>\n      <td>17</td>\n      <td>0.289806</td>\n      <td>0.360373</td>\n      <td>0.289806</td>\n      <td>0.360373</td>\n      <td>0.360373</td>\n      <td>0.289806</td>\n      <td>0.033493</td>\n      <td>5.107243</td>\n      <td>2</td>\n      <td>{'feat': 1, 'left': {'feat': 6, 'left': {'valu...</td>\n    </tr>\n    <tr>\n      <th>3464</th>\n      <td>100000</td>\n      <td>7</td>\n      <td>99</td>\n      <td>36054.304688</td>\n      <td>-171.508636</td>\n      <td>0.918660</td>\n      <td>0.826923</td>\n      <td>17</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.360373</td>\n      <td>0.360373</td>\n      <td>0.286508</td>\n      <td>0.063796</td>\n      <td>5.600595</td>\n      <td>2</td>\n      <td>{'feat': 1, 'left': {'feat': 4, 'left': {'feat...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3465 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pandas.read_csv(\"save/lawsuit_k_supp2.csv\")\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "for leaf_limit in range (1, 4):\n",
    "    result['accuracy_relab_'+str(leaf_limit)] = np.nan\n",
    "    result['discrimination_relab_'+str(leaf_limit)] = np.nan\n",
    "    result['discrimination_relab_abs_'+str(leaf_limit)] = np.nan\n",
    "    result['discrimination_relab_test_'+str(leaf_limit)] = np.nan\n",
    "    result['discrimination_relab_test_abs_'+str(leaf_limit)] = np.nan\n",
    "\n",
    "for threshold in [0.0, 0.1, 0.2, 0.3]:\n",
    "    result['accuracy_relab_'+str(threshold)] = np.nan\n",
    "    result['discrimination_relab_'+str(threshold)] = np.nan\n",
    "    result['discrimination_relab_abs_'+str(threshold)] = np.nan\n",
    "    result['discrimination_relab_test_'+str(threshold)] = np.nan\n",
    "    result['discrimination_relab_test_abs_'+str(threshold)] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3465/3465 [02:43<00:00, 21.17it/s]\n",
      "100%|██████████| 3465/3465 [03:12<00:00, 17.96it/s]\n",
      "100%|██████████| 3465/3465 [03:08<00:00, 18.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "k                                                                                        100000\ndepth                                                                                         3\nrandom_state                                                                                  1\nclf.error_                                                                          32585.90625\nclf.accuracy_                                                                       -154.913422\naccuracy_pred                                                                          0.885167\naccuracy_test                                                                          0.846154\nsum_misclassified_train                                                                      24\nsum_discrimination_additive_train_abs                                                  0.308857\nsum_discrimination_additive_train_pred_abs                                             0.325619\nsum_discrimination_additive_train                                                      0.308857\nsum_discrimination_additive_train_pred                                                 0.325619\ndiscrimination_train                                                                   0.325619\ndiscrimination_train_pred                                                              0.308857\ndiscrimination_test                                                                    0.375758\nduration                                                                               0.395489\nmin_supp                                                                                      2\nclf.tree_                                     {'feat': 9, 'left': {'feat': 3, 'left': {'feat...\naccuracy_relab_1                                                                        0.85645\ndiscrimination_relab_1                                                                  0.22647\ndiscrimination_relab_abs_1                                                              0.39123\ndiscrimination_relab_test_1                                                            0.169697\ndiscrimination_relab_test_abs_1                                                        0.581818\naccuracy_relab_2                                                                        0.84688\ndiscrimination_relab_2                                                                  0.21047\ndiscrimination_relab_abs_2                                                              0.37523\ndiscrimination_relab_test_2                                                            0.169697\ndiscrimination_relab_test_abs_2                                                        0.581818\naccuracy_relab_3                                                                        0.77033\ndiscrimination_relab_3                                                                  0.10628\ndiscrimination_relab_abs_3                                                              0.27104\ndiscrimination_relab_test_3                                                            0.160606\ndiscrimination_relab_test_abs_3                                                        0.572727\naccuracy_relab_0.0                                                                          NaN\ndiscrimination_relab_0.0                                                                    NaN\ndiscrimination_relab_abs_0.0                                                                NaN\ndiscrimination_relab_test_0.0                                                               NaN\ndiscrimination_relab_test_abs_0.0                                                           NaN\naccuracy_relab_0.1                                                                          NaN\ndiscrimination_relab_0.1                                                                    NaN\ndiscrimination_relab_abs_0.1                                                                NaN\ndiscrimination_relab_test_0.1                                                               NaN\ndiscrimination_relab_test_abs_0.1                                                           NaN\naccuracy_relab_0.2                                                                          NaN\ndiscrimination_relab_0.2                                                                    NaN\ndiscrimination_relab_abs_0.2                                                                NaN\ndiscrimination_relab_test_0.2                                                               NaN\ndiscrimination_relab_test_abs_0.2                                                           NaN\naccuracy_relab_0.3                                                                          NaN\ndiscrimination_relab_0.3                                                                    NaN\ndiscrimination_relab_abs_0.3                                                                NaN\ndiscrimination_relab_test_0.3                                                               NaN\ndiscrimination_relab_test_abs_0.3                                                           NaN\nName: 1000, dtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "def round_down(a):\n",
    "    return math.floor(a * 100000)/100000.0\n",
    "clf = DL85Classifier(max_depth=1, error_function=lambda tids: utils.misclassified(list(tids), y_train), min_sup=2, time_limit=600)\n",
    "clf.fit(X_train)\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    for leaf_limit in range (1, 4):\n",
    "        for index, row in tqdm(result.iterrows(), total=result.shape[0]):\n",
    "            #if (index in [1551, 1573, 1591, 1595, 1596, 1597] or index < 1900) and False:\n",
    "            #    continue\n",
    "            random_state = result.at[index, 'random_state']\n",
    "            accuracy_train = result.at[index, 'accuracy_pred']\n",
    "            discrimination_train_pred = result.at[index, 'discrimination_train_pred']\n",
    "\n",
    "            X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = utils.train_test_split(random_state, X, y, sensitive)\n",
    "            clf.tree_ = ast.literal_eval(result.at[index, 'clf.tree_'])\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "\n",
    "            leafs_relab = relabeling.relab_leaf_limit(clf.tree_, y_train, y_pred_train, sensitive_train, leaf_limit)\n",
    "            sum_acc = 0\n",
    "            sum_disc = 0\n",
    "            for leaf in leafs_relab:\n",
    "                relabeling.browse_and_relab(clf.tree_, list(leaf.path), leaf)\n",
    "                sum_acc += leaf.acc\n",
    "                sum_disc += leaf.disc\n",
    "            y_pred_train_relab = clf.predict(X_train)\n",
    "            discrimation_train_pred_relab = round_down(relabeling.discrimination(y_train, y_pred_train_relab, sensitive_train))\n",
    "            result.at[index, 'discrimination_relab_'+str(leaf_limit)] = discrimation_train_pred_relab\n",
    "            accuracy_train_relab = round_down(accuracy_score(y_train, y_pred_train_relab))\n",
    "            result.at[index, 'accuracy_relab_'+str(leaf_limit)] = accuracy_train_relab\n",
    "\n",
    "            utils.tree_upgrade(clf.tree_, y_train, y_pred_train_relab, sensitive_train)\n",
    "            sum_discri_ = round_down(utils.sum_elem_tree(ast.literal_eval(str(clf.tree_)), 'discrimination_additive_pred', do_abs=False))\n",
    "            sum_discri_abs_ = round_down(utils.sum_elem_tree(ast.literal_eval(str(clf.tree_)), 'discrimination_additive_pred', do_abs=True))\n",
    "            result.at[index, 'discrimination_relab_'+str(leaf_limit)] = sum_discri_\n",
    "            result.at[index, 'discrimination_relab_abs_'+str(leaf_limit)] = sum_discri_abs_\n",
    "\n",
    "            y_pred_test_relab = clf.predict(X_test)\n",
    "            discri_test= list()\n",
    "            utils.get_discri_test(clf.tree_, X_test, y_pred_test_relab, sensitive_test, discri_test, X.columns, path=None)\n",
    "            sum_discrimination_additive_test_pred = sum(discri_test)\n",
    "            sum_discrimination_additive_test_pred_abs = 0\n",
    "            [sum_discrimination_additive_test_pred_abs := sum_discrimination_additive_test_pred_abs + abs(d) for d in discri_test]\n",
    "\n",
    "            result.at[index, 'discrimination_relab_test_'+str(leaf_limit)] = sum_discrimination_additive_test_pred\n",
    "            result.at[index, 'discrimination_relab_test_abs_'+str(leaf_limit)] = sum_discrimination_additive_test_pred_abs\n",
    "\n",
    "\n",
    "            new_acc= round_down(accuracy_train+sum_acc)\n",
    "            new_disc = round_down(discrimination_train_pred+sum_disc)\n",
    "            if not (round_down(new_acc) == round_down(accuracy_train_relab) or round_down(new_disc)== round_down(discrimation_train_pred_relab)):\n",
    "                print(index)\n",
    "                print(result.iloc[index])\n",
    "                print(f\"{relabeling.discrimination(y_train, y_pred_train_relab, sensitive_train)}\")\n",
    "                print()\n",
    "\n",
    "                for leaf in leafs_relab:\n",
    "                    print(leaf)\n",
    "                print(f\"Accuracy:\\n\"\n",
    "                      f\"    Before      : {accuracy_train}\\n\"\n",
    "                      f\"    Leafs       : {sum_acc}\\n\"\n",
    "                      f\"    After       : {accuracy_train_relab}\\n\"\n",
    "                      f\"    Before+Leafs: {new_acc}\\n\"\n",
    "                      f\"    Check       : {round_down(new_acc) == round_down(accuracy_train_relab)}\\n\"\n",
    "                      f\"    Real diff   : {accuracy_train - accuracy_train_relab}\")\n",
    "                print(f\"Discrim:\\n\"\n",
    "                      f\"    Before      : {discrimination_train_pred}\\n\"\n",
    "                      f\"    Leafs       : {sum_disc }\\n\"\n",
    "                      f\"    After       : {discrimation_train_pred_relab}\\n\"\n",
    "                      f\"    Before+Leafs: {new_disc}\\n\"\n",
    "                      f\"    Check       : {round_down(new_disc)== round_down(discrimation_train_pred_relab)}\\n\"\n",
    "                      f\"    Real diff   : {discrimination_train_pred - discrimation_train_pred_relab}\")\n",
    "                display(graphviz.Source(utils.export_graphviz(clf)))\n",
    "\n",
    "                assert(round_down(new_acc) == round_down(accuracy_train_relab))\n",
    "                assert(round_down(new_disc)== round_down(discrimation_train_pred_relab))\n",
    "        result.to_csv('save/'+FILE_NAME+'.csv', index=False)\n",
    "    display(result.iloc[1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3465/3465 [02:42<00:00, 21.33it/s]\n",
      "100%|██████████| 3465/3465 [03:29<00:00, 16.51it/s]\n",
      "100%|██████████| 3465/3465 [04:00<00:00, 14.42it/s]\n",
      " 63%|██████▎   | 2199/3465 [01:43<01:20, 15.70it/s]"
     ]
    }
   ],
   "source": [
    "if run:\n",
    "    for threshold in [0.0, 0.1, 0.2, 0.3]:\n",
    "        for index, row in tqdm(result.iterrows(), total=result.shape[0]):\n",
    "\n",
    "            random_state = result.at[index, 'random_state']\n",
    "            accuracy_train = result.at[index, 'accuracy_pred']\n",
    "            discrimination_train_pred = result.at[index, 'discrimination_train_pred']\n",
    "\n",
    "            X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = utils.train_test_split(random_state, X, y, sensitive)\n",
    "            clf.tree_ = ast.literal_eval(result.at[index, 'clf.tree_'])\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "\n",
    "            leafs_relab = relabeling.relab(clf.tree_, y_train, y_pred_train, sensitive_train, threshold)\n",
    "            sum_acc = 0\n",
    "            sum_disc = 0\n",
    "            for leaf in leafs_relab:\n",
    "                relabeling.browse_and_relab(clf.tree_, list(leaf.path), leaf)\n",
    "                sum_acc += leaf.acc\n",
    "                sum_disc += leaf.disc\n",
    "            y_pred_train_relab = clf.predict(X_train)\n",
    "            discrimation_train_pred_relab = round_down(relabeling.discrimination(y_train, y_pred_train_relab, sensitive_train))\n",
    "            result.at[index, 'discrimination_relab_'+str(threshold)] = discrimation_train_pred_relab\n",
    "            accuracy_train_relab = round_down(accuracy_score(y_train, y_pred_train_relab))\n",
    "            result.at[index, 'accuracy_relab_'+str(threshold)] = accuracy_train_relab\n",
    "\n",
    "            utils.tree_upgrade(clf.tree_, y_train, y_pred_train_relab, sensitive_train)\n",
    "            sum_discri_ = round_down(utils.sum_elem_tree(ast.literal_eval(str(clf.tree_)), 'discrimination_additive_pred', do_abs=False))\n",
    "            sum_discri_abs_ = round_down(utils.sum_elem_tree(ast.literal_eval(str(clf.tree_)), 'discrimination_additive_pred', do_abs=True))\n",
    "            result.at[index, 'discrimination_relab_'+str(threshold)] = sum_discri_\n",
    "            result.at[index, 'discrimination_relab_abs_'+str(threshold)] = sum_discri_abs_\n",
    "\n",
    "            y_pred_test_relab = clf.predict(X_test)\n",
    "            discri_test= list()\n",
    "            utils.get_discri_test(clf.tree_, X_test, y_pred_test_relab, sensitive_test, discri_test, X.columns, path=None)\n",
    "            sum_discrimination_additive_test_pred = sum(discri_test)\n",
    "            sum_discrimination_additive_test_pred_abs = 0\n",
    "            [sum_discrimination_additive_test_pred_abs := sum_discrimination_additive_test_pred_abs + abs(d) for d in discri_test]\n",
    "\n",
    "            result.at[index, 'discrimination_relab_test_'+str(threshold)] = sum_discrimination_additive_test_pred\n",
    "            result.at[index, 'discrimination_relab_test_abs_'+str(threshold)] = sum_discrimination_additive_test_pred_abs\n",
    "\n",
    "            new_acc= round_down(accuracy_train+sum_acc)\n",
    "            new_disc = round_down(discrimination_train_pred+sum_disc)\n",
    "            if not ( discrimation_train_pred_relab <= threshold or round_down(new_acc) == round_down(accuracy_train_relab) or round_down(new_disc)== round_down(discrimation_train_pred_relab)):\n",
    "                print(index)\n",
    "                print(result.iloc[index])\n",
    "                print(f\"{discrimation_train_pred_relab}\")\n",
    "                print()\n",
    "\n",
    "                for leaf in leafs_relab:\n",
    "                    print(leaf)\n",
    "                print(f\"Accuracy:\\n\"\n",
    "                      f\"    Before      : {accuracy_train}\\n\"\n",
    "                      f\"    Leafs       : {sum_acc}\\n\"\n",
    "                      f\"    After       : {accuracy_train_relab}\\n\"\n",
    "                      f\"    Before+Leafs: {new_acc}\\n\"\n",
    "                      f\"    Check       : {round_down(new_acc) == round_down(accuracy_train_relab)}\\n\"\n",
    "                      f\"    Real diff   : {accuracy_train - accuracy_train_relab}\")\n",
    "                print(f\"Discrim:\\n\"\n",
    "                      f\"    Before      : {discrimination_train_pred}\\n\"\n",
    "                      f\"    Leafs       : {sum_disc }\\n\"\n",
    "                      f\"    After       : {discrimation_train_pred_relab}\\n\"\n",
    "                      f\"    Before+Leafs: {new_disc}\\n\"\n",
    "                      f\"    Check       : {round_down(new_disc)== round_down(discrimation_train_pred_relab)}\\n\"\n",
    "                      f\"    Real diff   : {discrimination_train_pred - discrimation_train_pred_relab}\")\n",
    "                display(graphviz.Source(utils.export_graphviz(clf)))\n",
    "                assert(relabeling.discrimination(y_train, y_pred_train_relab, sensitive_train) <= threshold)\n",
    "                assert(round_down(new_acc) == round_down(accuracy_train_relab))\n",
    "                assert(round_down(new_disc)== round_down(discrimation_train_pred_relab))\n",
    "\n",
    "        result.to_csv('save/'+FILE_NAME+'.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = pandas.read_csv('save/'+FILE_NAME+'.csv')\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_COLORS11 = ['#8b4513', '#006400', '#4682b4', '#4b0082', '#ff0000', '#00ff7f', '#00ffff', '#0000ff', '#ffff54',\n",
    "             '#ff1493', '#ffe4c4']\n",
    "_COLORS4 = ['#ff0000', '#00ff00', '#0000ff', '#87cefa']\n",
    "_COLORS5 = ['#008080', '#ffa500', '#00ff00', '#0000ff', '#ff1493']\n",
    "_COLORS7 = ['#2e8b57', '#ff0000', '#ffd700', '#c71585', '#00ff00', '#0000ff', '#1e90ff']\n",
    "_COLORS10 = ['#006400', '#00008b', '#b03060', '#ff4500', '#ffd700', '#7cfc00', '#00ffff', '#ff00ff', '#6495ed',\n",
    "          '#ffdab9']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "legends = ['no relab', 'relab 1 leaf', 'relab 2 leafs', 'relab 3 leafs']\n",
    "fig, axes = plt.subplots()\n",
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS4.copy()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "    for x_axe, y_axe in [('accuracy_train', 'discrimination_train_pred'), ('accuracy_relab_1', 'discrimination_relab_1'), ('accuracy_relab_2', 'discrimination_relab_2'), ('accuracy_relab_3', 'discrimination_relab_3')]:\n",
    "        index_color = 0\n",
    "        y_values = list()\n",
    "        x_values = list()\n",
    "        for k in result['k'].unique():\n",
    "            tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "            x_values.append(tmp[x_axe].mean())\n",
    "            y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "            plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors11[index_color])\n",
    "            \"\"\"\n",
    "            plt.annotate(k, # this is the text\n",
    "                         (tmp[x_axe].mean(), tmp[y_axe].mean()), # these are the coordinates to position the label\n",
    "                         textcoords=\"offset points\", # how to position the text\n",
    "                         xytext=(0,10), # distance from text to points (x,y)\n",
    "                         ha='center')\n",
    "            \"\"\"\n",
    "            index_color +=1\n",
    "\n",
    "        plt.plot(x_values, y_values, label=legends[index_legends], c=colors4[index_legends])\n",
    "        index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('discrimination')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.ylim(-0.2, 0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS5.copy()\n",
    "legends = ['relab discrim < 0', 'relab discrim < 0.1', 'relab discrim < 0.2', 'relab discrim < 0.3', 'no relab']\n",
    "fig, axes = plt.subplots()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "    for x_axe, y_axe in [('accuracy_relab_0.0', 'discrimination_relab_0.0'), ('accuracy_relab_0.1', 'discrimination_relab_0.1'), ('accuracy_relab_0.2', 'discrimination_relab_0.2'), ('accuracy_relab_0.3', 'discrimination_relab_0.3'), ('accuracy_train', 'discrimination_train_pred')]:\n",
    "        index_color = 0\n",
    "        y_values = list()\n",
    "        x_values = list()\n",
    "        for k in result['k'].unique():\n",
    "            tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "            x_values.append(tmp[x_axe].mean())\n",
    "            y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "            plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors11[index_color])\n",
    "            \"\"\"\n",
    "            plt.annotate(k, # this is the text\n",
    "                         (tmp[x_axe].mean(), tmp[y_axe].mean()), # these are the coordinates to position the label\n",
    "                         textcoords=\"offset points\", # how to position the text\n",
    "                         xytext=(0,10), # distance from text to points (x,y)\n",
    "                         ha='center')\n",
    "            \"\"\"\n",
    "            index_color +=1\n",
    "\n",
    "        plt.plot(x_values, y_values, label=legends[index_legends], c=colors4[index_legends])\n",
    "        index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('discrimination')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.ylim(-0.2, 0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors = _COLORS7.copy()\n",
    "legends = ['Depth 1', 'Depth 2', 'Depth 3', 'Depth 4', 'Depth 5', 'Depth 6', 'Depth 7']\n",
    "markers = ['o', 'x', 's', 'd', 'p', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X']\n",
    "\n",
    "index_legend = 0\n",
    "index_color = 0\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "k=1\n",
    "for depth in result['depth'].unique():\n",
    "    y_values = list()\n",
    "    x_values = list()\n",
    "    index_marker = 0\n",
    "    for x_axe, y_axe in [('accuracy_relab_0.0', 'discrimination_relab_0.0'), ('accuracy_relab_0.1', 'discrimination_relab_0.1'), ('accuracy_relab_0.2', 'discrimination_relab_0.2'), ('accuracy_relab_0.3', 'discrimination_relab_0.3'), ('accuracy_train', 'discrimination_train_pred')]:\n",
    "\n",
    "        tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "        x_values.append(tmp[x_axe].mean())\n",
    "        y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "        plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors[index_color], marker=markers[index_marker])\n",
    "        index_marker +=1\n",
    "\n",
    "    plt.plot(x_values, y_values, label=legends[index_legend], c=colors[index_color])\n",
    "    index_color +=1\n",
    "    index_legend +=1\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "plt.title(f'Threshold-Depth')\n",
    "plt.ylabel('discrimination')\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylim(-0.2, 0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors = _COLORS7.copy()\n",
    "legends = ['Depth 1', 'Depth 2', 'Depth 3', 'Depth 4', 'Depth 5', 'Depth 6', 'Depth 7']\n",
    "markers = ['o', 'x', 's', 'd', 'p', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X']\n",
    "\n",
    "index_legend = 0\n",
    "index_color = 0\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "k=1\n",
    "for depth in result['depth'].unique():\n",
    "    y_values = list()\n",
    "    x_values = list()\n",
    "    index_marker = 0\n",
    "    for x_axe, y_axe in [('accuracy_train', 'discrimination_train_pred'), ('accuracy_relab_1', 'discrimination_relab_1'), ('accuracy_relab_2', 'discrimination_relab_2'), ('accuracy_relab_3', 'discrimination_relab_3')]:\n",
    "\n",
    "        tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "        x_values.append(tmp[x_axe].mean())\n",
    "        y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "        plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors[index_color], marker=markers[index_marker])\n",
    "        index_marker +=1\n",
    "\n",
    "    plt.plot(x_values, y_values, label=legends[index_legend], c=colors[index_color])\n",
    "    index_color +=1\n",
    "    index_legend +=1\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "plt.title(f'Leaf-Depth')\n",
    "plt.ylabel('discrimination')\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylim(-0.2, 0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS5.copy()\n",
    "legends = ['relab discrim < 0', 'relab discrim < 0.1', 'relab discrim < 0.2', 'relab discrim < 0.3', 'no relab']\n",
    "fig, axes = plt.subplots()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "    for x_axe, y_axe in [('accuracy_relab_0.0', 'sum_discri_0.0'), ('accuracy_relab_0.1', 'sum_discri_0.1'), ('accuracy_relab_0.2', 'sum_discri_0.2'), ('accuracy_relab_0.3', 'sum_discri_0.3'), ('accuracy_train', 'sum_discrimination_additive_train_pred')]:\n",
    "        index_color = 0\n",
    "        y_values = list()\n",
    "        x_values = list()\n",
    "        for k in result['k'].unique():\n",
    "            tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "            x_values.append(tmp[x_axe].mean())\n",
    "            y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "            plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors11[index_color])\n",
    "            \"\"\"\n",
    "            plt.annotate(k, # this is the text\n",
    "                         (tmp[x_axe].mean(), tmp[y_axe].mean()), # these are the coordinates to position the label\n",
    "                         textcoords=\"offset points\", # how to position the text\n",
    "                         xytext=(0,10), # distance from text to points (x,y)\n",
    "                         ha='center')\n",
    "            \"\"\"\n",
    "            index_color +=1\n",
    "\n",
    "        plt.plot(x_values, y_values, label=legends[index_legends], c=colors4[index_legends])\n",
    "        index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('sum_discrimination_')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.ylim(-0.2, 0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS5.copy()\n",
    "legends = ['relab discrim < 0', 'relab discrim < 0.1', 'relab discrim < 0.2', 'relab discrim < 0.3', 'no relab']\n",
    "fig, axes = plt.subplots()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "    for x_axe, y_axe in [('accuracy_relab_0.0', 'sum_discri_abs_0.0'), ('accuracy_relab_0.1', 'sum_discri_abs_0.1'), ('accuracy_relab_0.2', 'sum_discri_abs_0.2'), ('accuracy_relab_0.3', 'sum_discri_abs_0.3'), ('accuracy_train', 'sum_discrimination_additive_pred_abs')]:\n",
    "        index_color = 0\n",
    "        y_values = list()\n",
    "        x_values = list()\n",
    "        for k in result['k'].unique():\n",
    "            tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "            x_values.append(tmp[x_axe].mean())\n",
    "            y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "            plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors11[index_color])\n",
    "            \"\"\"\n",
    "            plt.annotate(k, # this is the text\n",
    "                         (tmp[x_axe].mean(), tmp[y_axe].mean()), # these are the coordinates to position the label\n",
    "                         textcoords=\"offset points\", # how to position the text\n",
    "                         xytext=(0,10), # distance from text to points (x,y)\n",
    "                         ha='center')\n",
    "            \"\"\"\n",
    "            index_color +=1\n",
    "\n",
    "        plt.plot(x_values, y_values, label=legends[index_legends], c=colors4[index_legends])\n",
    "        index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('sum_discrimination_abs')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS5.copy()\n",
    "legends = ['relab discrim < 0', 'relab discrim < 0.1', 'relab discrim < 0.2', 'relab discrim < 0.3', 'no relab']\n",
    "fig, axes = plt.subplots()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "    for x_axe, y_axe in [('sum_discri_0.0', 'sum_discri_abs_0.0'), ('sum_discri_0.1', 'sum_discri_abs_0.1'), ('sum_discri_0.2', 'sum_discri_abs_0.2'), ('sum_discri_0.3', 'sum_discri_abs_0.3'), ('sum_discrimination_additive_train_abs', 'sum_discrimination_additive_pred_abs')]:\n",
    "        index_color = 0\n",
    "        y_values = list()\n",
    "        x_values = list()\n",
    "        for k in result['k'].unique():\n",
    "            tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth)]\n",
    "            x_values.append(tmp[x_axe].mean())\n",
    "            y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "            plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors11[index_color])\n",
    "            \"\"\"\n",
    "            plt.annotate(k, # this is the text\n",
    "                         (tmp[x_axe].mean(), tmp[y_axe].mean()), # these are the coordinates to position the label\n",
    "                         textcoords=\"offset points\", # how to position the text\n",
    "                         xytext=(0,10), # distance from text to points (x,y)\n",
    "                         ha='center')\n",
    "            \"\"\"\n",
    "            index_color +=1\n",
    "\n",
    "        plt.plot(x_values, y_values, label=legends[index_legends], c=colors4[index_legends])\n",
    "        index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('sum_discrimination_abs')\n",
    "    plt.xlabel('sum_discrimination')\n",
    "    plt.ylim(-0.2, 1)\n",
    "    plt.xlim(-0.2, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k=0\n",
    "\n",
    "colors11 = _COLORS11.copy()\n",
    "colors4 = _COLORS5.copy()\n",
    "colors7 = _COLORS7.copy()\n",
    "fig, axes = plt.subplots()\n",
    "for depth in result['depth'].unique():\n",
    "    index_legends = 0\n",
    "   # for k in df['k'].unique():#df['k'].unique():\n",
    "    #for k in [0, 10, 250, 5000, 100000]:#df['k'].unique():\n",
    "    index_color = 0\n",
    "    y_values = list()\n",
    "    x_values = list()\n",
    "    for x_axe, y_axe in [('sum_discri_0.0', 'sum_discri_abs_0.0'), ('sum_discri_0.1', 'sum_discri_abs_0.1'), ('sum_discri_0.2', 'sum_discri_abs_0.2'), ('sum_discri_0.3', 'sum_discri_abs_0.3'), ('sum_discrimination_additive_train_abs', 'sum_discrimination_additive_pred_abs')]:\n",
    "\n",
    "\n",
    "\n",
    "        tmp = result.loc[(result[\"k\"] == k) & (result[\"depth\"] == depth) & (result[\"k\"] == 0)]\n",
    "        x_values.append(tmp[x_axe].mean())\n",
    "        y_values.append(tmp[y_axe].mean())\n",
    "\n",
    "        plt.scatter(tmp[x_axe].mean(), tmp[y_axe].mean(), c=colors4[index_color])\n",
    "        index_color +=1\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(x_values, y_values, label=\"k=\"+str(k), c=colors11[index_legends])\n",
    "    index_legends +=1\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(f'Depth {depth}')\n",
    "    plt.ylabel('sum_discrimination_abs')\n",
    "    plt.xlabel('sum_discrimination')\n",
    "    plt.ylim(-0.2, 1)\n",
    "    plt.xlim(-0.2, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}